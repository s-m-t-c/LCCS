{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import datacube\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gp\n",
    "import fiona\n",
    "from datacube import helpers\n",
    "\n",
    "import rasterio\n",
    "import sklearn\n",
    "import graphviz \n",
    "import pdb\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# Import external functions from dea-notebooks using relative link to 10_Scripts\n",
    "sys.path.append('/g/data/u46/users/sc0554/dea-notebooks/Scripts')\n",
    "from dea_bandindices import calculate_indices\n",
    "import dea_classificationtools\n",
    "from dea_plotting import display_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Wrappers' to translate xarrays to np arrays and back for interfacing with sklearn models\n",
    "def sklearn_flatten(input_xr):\n",
    "    \"\"\"\n",
    "    Reshape a DataArray or Dataset with spatial (and optionally temporal) structure into \n",
    "    an np.array with the spatial and temporal dimensions flattened into one dimension.\n",
    "    \n",
    "    This flattening procedure enables DataArrays and Datasets to be used to train and predict\n",
    "    with sklearn models.\n",
    "    \n",
    "    Last modified: September 2019\n",
    "    \n",
    "    Parameters\n",
    "    ----------  \n",
    "        input_xr : a DataArray or Dataset. Must have dimensions 'x' and 'y', may have dimension 'time'.\n",
    "                   Dimensions other than 'x', 'y' and 'time' are unaffected by the flattening.\n",
    "                   \n",
    "    Returns\n",
    "    ----------\n",
    "        input_np : a numpy array corresponding to input_xr.data (or input_xr.to_array().data), with\n",
    "                   dimensions 'x','y' and 'time' flattened into a single dimension, which is the first\n",
    "                   axis of the returned array. input_np contains no NaNs.\n",
    "    \n",
    "    \"\"\"\n",
    "#     pdb.set_trace()\n",
    "    #cast input Datasets to DataArray\n",
    "    if isinstance(input_xr,xr.Dataset):\n",
    "        input_xr = input_xr.to_array()\n",
    "    \n",
    "    #stack across pixel dimensions, handling timeseries if necessary\n",
    "    if 'time' in input_xr.dims:\n",
    "        stacked = input_xr.stack(z=['x','y','time'])\n",
    "    else:\n",
    "        stacked = input_xr.stack(z=['x','y'])\n",
    "        \n",
    "    #finding 'bands' dimensions in each pixel - these will not be flattened as their context is important for sklearn\n",
    "    pxdims = []\n",
    "    for dim in stacked.dims:\n",
    "        if dim != 'z':\n",
    "            pxdims.append(dim)\n",
    "    \n",
    "    #mask NaNs - we mask pixels with NaNs in *any* band, because sklearn cannot accept NaNs as input\n",
    "    mask = np.isnan(stacked)\n",
    "    if len(pxdims)!=0:\n",
    "        mask = mask.any(dim=pxdims)\n",
    "        \n",
    "    #turn the mask into a numpy array (boolean indexing with xarrays acts weird)\n",
    "    mask=mask.data\n",
    "    #the dimension we are masking along ('z') needs to be the first dimension in the underlying np array for\n",
    "    #the boolean indexing to work\n",
    "    stacked = stacked.transpose('z',*pxdims)\n",
    "    input_np = stacked.data[~mask]\n",
    "    \n",
    "    return input_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app = 'classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the shapefile containing labelled data\n",
    "# shp_path='/g/data1a/u46/users/sc0554/LCCS/LCCS/decision_tree/training_data/training_samples_2015_-11_-35.shp'\n",
    "# shp_path='/g/data1a/u46/users/sc0554/LCCS/LCCS/decision_tree/training_data/LANDSCAPE_SALandCover_TrainingData_PointsConsolidated_SAOnly.shp'\n",
    "shp_path = '/g/data1a/r78/LCCS_Aberystwyth/training_data/Cell_-13_-36_2015.shp'\n",
    "shapes=fiona.open(shp_path,'r')\n",
    "# shp = gp.read_file(shp_path)\n",
    "crs=geometry.CRS(shapes.crs_wkt)\n",
    "# field = 'Classvalue'\n",
    "# field = 'CLASS_VALU'\n",
    "field = 'classnum'\n",
    "product = 'ls8_nbart_geomedian_annual'\n",
    "query = {\n",
    "         'time': ('2015-01-01', '2015-02-01')\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affine of single pixel is nans fyi\n",
    "\n",
    "feature_rast_list = []\n",
    "for feature in shapes:\n",
    "        \n",
    "        # Datacube load the data near each feature in a shapefile, this returns a square / rectamg;e\n",
    "        f_geometry=feature['geometry']\n",
    "        geom=geometry.Geometry(f_geometry,crs=crs)\n",
    "        query['geopolygon'] = geom\n",
    "        data = dc.load(product=product, group_by='solar_day', **query)\n",
    "        \n",
    "        # Rasterise the feature\n",
    "        mask = rasterize([(feature['geometry'], feature['properties'][field])],\n",
    "                                           out_shape=data.isel(time=0).blue.shape,\n",
    "                                           transform=data.affine\n",
    "                                          )\n",
    "        \n",
    "        mask = xr.DataArray(mask, coords=(data.y, data.x))\n",
    "\n",
    "        # Mask the data so area outside the shapefile is NaN\n",
    "        data = data.where(mask == feature['properties'][field], np.nan)\n",
    "        \n",
    "        # Calculate indices\n",
    "        data = calculate_indices(data, 'BUI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'BSI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'BSI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'NBI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'EVI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'NDWI', collection='ga_ls_2')\n",
    "        data = calculate_indices(data, 'MSAVI', collection='ga_ls_2')\n",
    "        \n",
    "        # Remove time step if present\n",
    "        data = data.isel(time=0)\n",
    "        \n",
    "        # Extract the label\n",
    "        label = feature['properties'][field]\n",
    "\n",
    "        # Append training data and label to list\n",
    "        feature_rast_list.append((data, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot should look like an the first feature of the training data with masking around it\n",
    "\n",
    "feature_rast_list[0][0].blue.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(data.data_vars)\n",
    "print(feature_names)\n",
    "target_names = np.array(('Natural Terrestrial Vegetated', 'Artificial Surface', 'Natural Surface', 'Artificail Water', 'Natural Water'))\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten arrays and append to list\n",
    "flat_train_list = []\n",
    "flat_val_list = []\n",
    "\n",
    "for feature in feature_rast_list:\n",
    "    # Flatten\n",
    "    flat_train = sklearn_flatten(feature[0])\n",
    "    \n",
    "    # Make a list of labels for the same length as the training data\n",
    "    flat_val = np.repeat(feature[1],flat_train.shape[0])\n",
    "    flat_train_list.append(flat_train)\n",
    "    flat_val_list.append(flat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack list of arrays into single array\n",
    "val_input = np.hstack(flat_val_list)\n",
    "train_input = np.vstack(flat_train_list)\n",
    "print(train_input.shape, val_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialise classifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# Fit classifier add \"==215\" to make a single class prediction.\n",
    "model = model.fit(train_input, val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECISION TREE\n",
    "\n",
    "from sklearn import tree\n",
    "# Initialise classifier\n",
    "model = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "# Fit classifier add \"==215\" to make a single class prediction.\n",
    "model = model.fit(train_input, val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEURAL NETWORK\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1, activation='relu')\n",
    "model = model.fit(train_input, val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the feature importance\n",
    "print(dict(zip(feature_names, dtree.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the structure of the tree\n",
    "plt.figure(figsize=(25,8))\n",
    "sklearn.tree.plot_tree(dtree, feature_names=feature_names, class_names=target_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the area you want to predict land cover here\n",
    "\n",
    "# Lake Eyre\n",
    "# x = (550000, 600000)\n",
    "# y = (-3000000, -2950000)\n",
    "# x = (-1000000, -950000)\n",
    "# y = (-3400000, -3350000)\n",
    "x = (-1200000, -1299850)\n",
    "y = (-3600000, -3500125)\n",
    "\n",
    "# # Coorong\n",
    "# x = (600000, 700000)\n",
    "# y = (-3950000, -3850000)\n",
    "\n",
    "query = {'time': ('2015-01-01', '2015-02-01')}\n",
    "query['x'] = (x[0], x[1])\n",
    "query['y'] = (y[0], y[1])\n",
    "query['crs'] = 'EPSG:3577'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_map(x, y, crs=\"EPSG:3577\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = dc.load(product=product, group_by='solar_day', **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = calculate_indices(new_data, 'BUI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'BSI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'BSI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'NBI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'EVI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'NDWI', collection='ga_ls_2')\n",
    "new_data = calculate_indices(new_data, 'MSAVI', collection='ga_ls_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = dea_classificationtools.predict_xr(model, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = predicted.isel(time=0).transpose()\n",
    "out = out.to_dataset(name=\"LCCS_L3\")\n",
    "out.attrs['crs']=geometry.CRS(data.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.write_geotiff('rforest_merged.tif', out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
